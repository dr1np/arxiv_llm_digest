import os
import json
import arxiv
import google.generativeai as genai
import openai
from datetime import datetime, timedelta, timezone

# --- é…ç½® ---
# 1. LLM æä¾›å•†: 'google' æˆ– 'openai'
LLM_PROVIDER = "google"  # <--- åœ¨è¿™é‡Œåˆ‡æ¢

# 2. API å¯†é’¥ (ä»ç¯å¢ƒå˜é‡è¯»å–)
#    è¿è¡Œ: export GOOGLE_API_KEY='ä½ çš„Google APIå¯†é’¥'
#    è¿è¡Œ: export OPENAI_API_KEY='ä½ çš„OpenAI APIå¯†é’¥'
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# 3. Arxiv æœç´¢é…ç½®, Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)
SEARCH_QUERY = 'cat:cs.CL OR cat:cs.AI OR cat:cs.LG'
MAX_RESULTS = 20  # æ¯å¤©å¤„ç†çš„æœ€å¤§è®ºæ–‡æ•°

# 4. LLM åˆ†æé…ç½®
# ä¸ºä¸åŒæä¾›å•†é€‰æ‹©åˆé€‚çš„æ¨¡å‹
MODEL_CONFIG = {
    "google": "gemini-2.5-flash",
    "openai": "gpt-5-2025-08-07"
}

PROMPT_TEMPLATE = """
You are a senior AI researcher specializing in Large Language Models.
Based on the title and abstract of the following paper, please perform these tasks:

**Paper Title:** {title}
**Paper Abstract:** {abstract}

**Tasks:**
1.  **Categorize** the paper into ONE of the following categories: 
    [Model Architecture, Training & Optimization, Data & Pre-training, Fine-tuning & Adaptation, Evaluation & Benchmarking, Multimodality, Applications, Safety & Ethics, Theory & Analysis, Other].
2.  **Summarize the main contribution** in a single, concise sentence.
3.  **Rate its potential novelty** on a scale of 1 to 5 (1=Incremental, 3=Interesting, 5=Potential Breakthrough).

Provide your response in a valid JSON format, like this:
{{"category": "...", "contribution": "...", "novelty": ...}}
"""

# 5. è¾“å‡ºæ–‡ä»¶
OUTPUT_FILE = "daily_arxiv_digest.md"
OUTPUT_DIR = "arxiv_digests_md"

# --- æ ¸å¿ƒåŠŸèƒ½ ---

def fetch_recent_papers():
    """ä» Arxiv è·å–è¿‡å»24å°æ—¶çš„è®ºæ–‡"""
    print("Fetching recent papers from arXiv...")
    # BUGFIX: ä½¿ç”¨å¸¦æ—¶åŒºçš„ UTC æ—¶é—´è¿›è¡Œæ¯”è¾ƒ
    yesterday_utc = datetime.now(timezone.utc) - timedelta(days=1)
    
    search = arxiv.Search(
        query=SEARCH_QUERY,
        max_results=MAX_RESULTS,
        sort_by=arxiv.SortCriterion.SubmittedDate
    )
    
    recent_papers = []
    for result in search.results():
        if result.published > yesterday_utc:
            recent_papers.append(result)
            
    print(f"Found {len(recent_papers)} new papers from the last 24 hours.")
    return recent_papers

def _analyze_with_google(model, paper):
    """ä½¿ç”¨ Google Gemini åˆ†æè®ºæ–‡"""
    prompt = PROMPT_TEMPLATE.format(title=paper.title, abstract=paper.summary)
    response = model.generate_content(prompt)
    # æ¸…ç†å¹¶è§£æ LLM å¯èƒ½è¿”å›çš„ markdown ä»£ç å—
    cleaned_response = response.text.strip().replace("```json", "").replace("```", "")
    return json.loads(cleaned_response)

def _analyze_with_openai(client, model_name, paper):
    """ä½¿ç”¨ OpenAI GPT åˆ†æè®ºæ–‡"""
    prompt = PROMPT_TEMPLATE.format(title=paper.title, abstract=paper.summary)
    response = client.chat.completions.create(
        model=model_name,
        messages=[
            {"role": "system", "content": "You are a helpful research assistant providing JSON output."},
            {"role": "user", "content": prompt}
        ],
        response_format={"type": "json_object"} # ç¡®ä¿è¿”å›JSON
    )
    return json.loads(response.choices[0].message.content)

def analyze_paper(provider, client, model_name, paper):
    """æ ¹æ®æä¾›å•†é€‰æ‹©åˆ†æå‡½æ•°"""
    print(f"  Analyzing with {provider}: {paper.title[:60]}...")
    try:
        if provider == 'google':
            return _analyze_with_google(client, paper)
        elif provider == 'openai':
            return _analyze_with_openai(client, model_name, paper)
    except Exception as e:
        print(f"    [!] Error analyzing paper: {e}")
        return None

def generate_markdown_report(analyzed_papers):
    """ç”Ÿæˆ Markdown æ ¼å¼çš„æŠ¥å‘Š"""
    if not analyzed_papers:
        return "# Daily arXiv LLM Digest\n\nNo new papers found today."

    analyzed_papers.sort(key=lambda x: x['analysis'].get('novelty', 0), reverse=True)
    
    recommendation = analyzed_papers[0]
    other_papers = analyzed_papers[1:]
    
    report_date = datetime.now().strftime("%Y-%m-%d")
    md_content = f"# Daily arXiv LLM Digest - {report_date}\n\n"
    md_content += f"Your daily summary of new papers on LLMs, analyzed by **{LLM_PROVIDER}**.\n\n"
    
    md_content += "## ğŸ”¥ Today's Top Recommendation\n\n"
    p = recommendation['paper']
    a = recommendation['analysis']
    md_content += f"### [{p.title}]({p.entry_id})\n"
    md_content += f"- **Authors**: {', '.join(author.name for author in p.authors)}\n"
    md_content += f"- **Category**: `{a.get('category', 'N/A')}`\n"
    md_content += f"- **Novelty Score**: `{a.get('novelty', 'N/A')}/5`\n"
    md_content += f"- **Contribution**: {a.get('contribution', 'N/A')}\n\n"
    # BUGFIX: ä½¿ç”¨ '\n' æ›¿æ¢æ¢è¡Œç¬¦ï¼Œè€Œä¸æ˜¯å­—æ¯ 'n'
    clean_summary = p.summary.replace('\n', ' ')
    md_content += f"**Abstract**: *{clean_summary}*\n\n"
    
    if other_papers:
        md_content += "---\n\n## ğŸ“š Other Papers Today\n\n"
        for item in other_papers:
            p = item['paper']
            a = item['analysis']
            md_content += f"### [{p.title}]({p.entry_id})\n"
            md_content += f"- **Category**: `{a.get('category', 'N/A')}` | **Novelty**: `{a.get('novelty', 'N/A')}/5`\n"
            md_content += f"- **Contribution**: {a.get('contribution', 'N/A')}\n\n"
            
    return md_content

def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    client = None
    model_name = MODEL_CONFIG[LLM_PROVIDER]

    print(f"Using LLM provider: {LLM_PROVIDER}")
    if LLM_PROVIDER == 'google':
        if not GOOGLE_API_KEY:
            raise ValueError("GOOGLE_API_KEY environment variable not set.")
        genai.configure(api_key=GOOGLE_API_KEY)
        client = genai.GenerativeModel(model_name)
    elif LLM_PROVIDER == 'openai':
        if not OPENAI_API_KEY:
            raise ValueError("OPENAI_API_KEY environment variable not set.")
        client = openai.OpenAI(api_key=OPENAI_API_KEY)
    else:
        raise ValueError(f"Unsupported LLM provider: {LLM_PROVIDER}")

    papers = fetch_recent_papers()
    if not papers:
        print("No new papers to process. Exiting.")
        return

    analyzed_papers = []
    for paper in papers:
        analysis = analyze_paper(LLM_PROVIDER, client, model_name, paper)
        if analysis:
            analyzed_papers.append({"paper": paper, "analysis": analysis})
    
    report = generate_markdown_report(analyzed_papers)

    # åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)
    
    # ç”Ÿæˆå¸¦æ—¥æœŸçš„æ–‡ä»¶å
    report_date = datetime.now().strftime("%Y-%m-%d")
    output_filename = os.path.join(OUTPUT_DIR, f"digest_{report_date}.md")

    with open(output_filename, "w", encoding="utf-8") as f:
        f.write(report)
        
    print(f"\nâœ… Digest report generated successfully: {output_filename}")
    if analyzed_papers:
        print("\n--- Today's Recommendation ---")
        # æå–æŠ¥å‘Šçš„æ¨èéƒ¨åˆ†è¿›è¡Œæ‰“å°
        recommendation_part = report.split("## ğŸ“š Other Papers Today")[0]
        print(recommendation_part)

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"An error occurred: {e}")